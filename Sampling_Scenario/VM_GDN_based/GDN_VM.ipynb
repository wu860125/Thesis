{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GDN Based VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "\n",
    "from util.env import get_device, set_device\n",
    "from util.preprocess import build_loc_net, construct_data\n",
    "from util.net_struct import get_feature_map, get_fc_graph_struc\n",
    "from util.iostream import printsep\n",
    "\n",
    "from datasets.WaferDataset import WaferDataset\n",
    "\n",
    "from models.GDN import GDN\n",
    "\n",
    "from run.train import train\n",
    "from run.test  import test\n",
    "from run.evaluate import get_err_scores, get_best_performance_data, get_val_performance_data, get_full_err_scores\n",
    "\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "<torch.cuda.device object at 0x000001D3DEF81780>\n",
      "NVIDIA GeForce RTX 3090 Ti\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "# check GPU situation\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to set random seed\n",
    "def setup_seed(seed):\n",
    "     random.seed(seed)\n",
    "     np.random.seed(seed)\n",
    "     if torch.cuda.is_available():\n",
    "          torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "     torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Main():\n",
    "    def __init__(self, train_config, env_config, debug=False):\n",
    "\n",
    "        self.train_config = train_config\n",
    "        self.env_config = env_config\n",
    "        self.datestr = None\n",
    "\n",
    "        dataset = self.env_config['dataset']\n",
    "        \n",
    "        sampled_orig, unsampled_orig, test_orig = self.get_orig_data(dataset)\n",
    "        sampled_train_orig = pd.concat([sampled_orig.iloc[i:i+316] for i in range(0, len(sampled_orig), 316*2)], ignore_index=True)\n",
    "        unsampled_train_orig = pd.concat([unsampled_orig.iloc[i:i+316] for i in range(316, len(unsampled_orig), 316*2)], ignore_index=True)\n",
    "\n",
    "        train, test = sampled_train_orig, unsampled_train_orig\n",
    "\n",
    "        if 'attack' in train.columns:\n",
    "            train = train.drop(columns=['attack'])\n",
    "\n",
    "        feature_map = get_feature_map(dataset)\n",
    "        fc_struc = get_fc_graph_struc(dataset)\n",
    "\n",
    "        set_device(env_config['device'])\n",
    "        self.device = get_device()\n",
    "\n",
    "        fc_edge_index = build_loc_net(fc_struc, list(train.columns), feature_map=feature_map)\n",
    "        fc_edge_index = torch.tensor(fc_edge_index, dtype = torch.long)\n",
    "\n",
    "        self.feature_map = feature_map\n",
    "\n",
    "        train_dataset_indata = construct_data(train, feature_map, labels=train.MRR.tolist(), mrrs=train.former_MRR.tolist())\n",
    "        test_dataset_indata = construct_data(test, feature_map, labels=test.MRR.tolist(), mrrs=train.former_MRR.tolist())\n",
    "\n",
    "        cfg = {'wafer_len': train_config['wafer_len']}\n",
    "\n",
    "        train_dataset = WaferDataset(train_dataset_indata, fc_edge_index, config=cfg)\n",
    "        test_dataset = WaferDataset(test_dataset_indata, fc_edge_index, config=cfg)\n",
    "\n",
    "\n",
    "        train_dataloader, val_dataloader = self.get_loaders(train_dataset, train_config['seed'], train_config['batch'], val_ratio = train_config['val_ratio'])\n",
    "\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.test_dataloader = DataLoader(test_dataset, batch_size=train_config['batch'],\n",
    "                            shuffle=False)\n",
    "\n",
    "\n",
    "        edge_index_sets = []\n",
    "        edge_index_sets.append(fc_edge_index)\n",
    "\n",
    "        self.model = GDN(edge_index_sets, len(feature_map), \n",
    "                dim=train_config['dim'], \n",
    "                input_dim=train_config['wafer_len'],\n",
    "                topk=train_config['topk'],\n",
    "            ).to(self.device)\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        if len(self.env_config['load_model_path']) > 0:\n",
    "            model_save_path = self.env_config['load_model_path']\n",
    "        else:\n",
    "            model_save_path = self.get_save_path()[0]\n",
    "\n",
    "            self.train_log = train(self.model, model_save_path, \n",
    "                config = self.train_config,\n",
    "                train_dataloader=self.train_dataloader,\n",
    "                val_dataloader=self.val_dataloader, \n",
    "                feature_map=self.feature_map,\n",
    "                test_dataloader=self.test_dataloader,\n",
    "                test_dataset=self.test_dataset,\n",
    "                train_dataset=self.train_dataset,\n",
    "                dataset_name=self.env_config['dataset']\n",
    "            )\n",
    "        \n",
    "        # test            \n",
    "        self.model.load_state_dict(torch.load(model_save_path))\n",
    "        best_model = self.model.to(self.device)\n",
    "\n",
    "        test_loss, test_result = test(best_model, self.test_dataloader)\n",
    "        val_loss, val_result = test(best_model, self.val_dataloader)\n",
    "\n",
    "        return test_loss, val_loss, test_result, val_result\n",
    "\n",
    "    def get_loaders(self, train_dataset, seed, batch, val_ratio=0.1):\n",
    "        dataset_len = int(len(train_dataset))\n",
    "        train_use_len = int(dataset_len * (1 - val_ratio))\n",
    "        val_use_len = int(dataset_len * val_ratio)\n",
    "        val_start_index = random.randrange(train_use_len)\n",
    "        indices = torch.arange(dataset_len)\n",
    "\n",
    "        train_sub_indices = torch.cat([indices[:val_start_index], indices[val_start_index+val_use_len:]])\n",
    "        train_subset = Subset(train_dataset, train_sub_indices)\n",
    "\n",
    "        val_sub_indices = indices[val_start_index:val_start_index+val_use_len]\n",
    "        val_subset = Subset(train_dataset, val_sub_indices)\n",
    "\n",
    "        train_dataloader = DataLoader(train_subset, batch_size=batch,\n",
    "                                shuffle=False)\n",
    "\n",
    "        val_dataloader = DataLoader(val_subset, batch_size=batch,\n",
    "                                shuffle=False)\n",
    "\n",
    "        return train_dataloader, val_dataloader\n",
    "\n",
    "    def get_save_path(self, feature_name=''):\n",
    "\n",
    "        dir_path = self.env_config['save_path']\n",
    "        \n",
    "        if self.datestr is None:\n",
    "            now = datetime.now()\n",
    "            self.datestr = now.strftime('%m_%d-%H_%M_%S')\n",
    "        datestr = self.datestr          \n",
    "\n",
    "        paths = [\n",
    "            f'./save_path/{dir_path}/best_{datestr}.pt',\n",
    "            f'./results/{dir_path}/{datestr}.csv',\n",
    "        ]\n",
    "\n",
    "        for path in paths:\n",
    "            dirname = os.path.dirname(path)\n",
    "            Path(dirname).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        return paths\n",
    "    \n",
    "    def get_orig_data(self, dataset):\n",
    "\n",
    "        train_orig = pd.read_csv(f'./data/{dataset}/train.csv', sep=',', index_col=0).reset_index(drop=True)\n",
    "        test_orig = pd.read_csv(f'./data/{dataset}/test.csv', sep=',', index_col=0).reset_index(drop=True)\n",
    "\n",
    "        short_sampled_orig = train_orig.iloc[316*2:, :].reset_index(drop=True)\n",
    "        former_mrr_sampled = train_orig.iloc[:-316*2, -1].rename('former_MRR').reset_index(drop=True)\n",
    "        addmrr_sampled_orig = pd.concat([short_sampled_orig, former_mrr_sampled], axis=1)\n",
    "\n",
    "        short_unsampled_orig = train_orig.iloc[316*1:, :].reset_index(drop=True)\n",
    "        former_mrr_unsampled = train_orig.iloc[:-316*1, -1].rename('former_MRR').reset_index(drop=True)\n",
    "        addmrr_unsampled_orig = pd.concat([short_unsampled_orig, former_mrr_unsampled], axis=1)\n",
    "\n",
    "        former_mrr_test = pd.concat([train_orig.iloc[-316*1:, -1], test_orig.iloc[:-316*1, -1]], ignore_index=True).rename(\"former_MRR\").reset_index(drop=True)\n",
    "        addmrr_test_orig = pd.concat([test_orig, former_mrr_test], axis=1)\n",
    "        \n",
    "        return addmrr_sampled_orig, addmrr_unsampled_orig, addmrr_test_orig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch (1 / 2000) (Train_loss:3389.28735352, ACU_loss:6778.57470703, Val_loss:3240.49389648)\n",
      "epoch (2 / 2000) (Train_loss:3363.42321777, ACU_loss:6726.84643555, Val_loss:3223.56250000)\n",
      "epoch (3 / 2000) (Train_loss:3340.14978027, ACU_loss:6680.29956055, Val_loss:3206.55322266)\n",
      "epoch (4 / 2000) (Train_loss:3315.61572266, ACU_loss:6631.23144531, Val_loss:3189.35424805)\n",
      "epoch (5 / 2000) (Train_loss:3289.64221191, ACU_loss:6579.28442383, Val_loss:3171.75830078)\n",
      "epoch (6 / 2000) (Train_loss:3261.93249512, ACU_loss:6523.86499023, Val_loss:3153.76562500)\n",
      "epoch (7 / 2000) (Train_loss:3232.44445801, ACU_loss:6464.88891602, Val_loss:3135.23510742)\n",
      "epoch (8 / 2000) (Train_loss:3202.51159668, ACU_loss:6405.02319336, Val_loss:3115.86010742)\n",
      "epoch (9 / 2000) (Train_loss:3169.18237305, ACU_loss:6338.36474609, Val_loss:3095.69042969)\n",
      "epoch (10 / 2000) (Train_loss:3133.28100586, ACU_loss:6266.56201172, Val_loss:3073.11743164)\n",
      "epoch (11 / 2000) (Train_loss:3094.18798828, ACU_loss:6188.37597656, Val_loss:3048.10083008)\n",
      "epoch (12 / 2000) (Train_loss:3051.61584473, ACU_loss:6103.23168945, Val_loss:3019.57226562)\n",
      "epoch (13 / 2000) (Train_loss:3007.32336426, ACU_loss:6014.64672852, Val_loss:2985.50317383)\n",
      "epoch (14 / 2000) (Train_loss:2957.18444824, ACU_loss:5914.36889648, Val_loss:2945.51489258)\n",
      "epoch (15 / 2000) (Train_loss:2901.82409668, ACU_loss:5803.64819336, Val_loss:2898.34594727)\n",
      "epoch (16 / 2000) (Train_loss:2840.16467285, ACU_loss:5680.32934570, Val_loss:2845.83544922)\n",
      "epoch (17 / 2000) (Train_loss:2773.59484863, ACU_loss:5547.18969727, Val_loss:2787.82519531)\n",
      "epoch (18 / 2000) (Train_loss:2700.35595703, ACU_loss:5400.71191406, Val_loss:2720.29907227)\n",
      "epoch (19 / 2000) (Train_loss:2620.24121094, ACU_loss:5240.48242188, Val_loss:2641.69165039)\n",
      "epoch (20 / 2000) (Train_loss:2535.21411133, ACU_loss:5070.42822266, Val_loss:2550.87866211)\n",
      "epoch (21 / 2000) (Train_loss:2441.40637207, ACU_loss:4882.81274414, Val_loss:2449.56225586)\n",
      "epoch (22 / 2000) (Train_loss:2337.37426758, ACU_loss:4674.74853516, Val_loss:2338.18896484)\n",
      "epoch (23 / 2000) (Train_loss:2227.88537598, ACU_loss:4455.77075195, Val_loss:2217.75878906)\n",
      "epoch (24 / 2000) (Train_loss:2110.08435059, ACU_loss:4220.16870117, Val_loss:2055.65527344)\n",
      "epoch (25 / 2000) (Train_loss:1980.51098633, ACU_loss:3961.02197266, Val_loss:1872.02380371)\n",
      "epoch (26 / 2000) (Train_loss:1841.27355957, ACU_loss:3682.54711914, Val_loss:1671.88940430)\n",
      "epoch (27 / 2000) (Train_loss:1694.19531250, ACU_loss:3388.39062500, Val_loss:1487.77587891)\n",
      "epoch (28 / 2000) (Train_loss:1542.22021484, ACU_loss:3084.44042969, Val_loss:1351.02722168)\n",
      "epoch (29 / 2000) (Train_loss:1384.71038818, ACU_loss:2769.42077637, Val_loss:1236.95336914)\n",
      "epoch (30 / 2000) (Train_loss:1222.76440430, ACU_loss:2445.52880859, Val_loss:1113.24182129)\n",
      "epoch (31 / 2000) (Train_loss:1060.77612305, ACU_loss:2121.55224609, Val_loss:963.14068604)\n",
      "epoch (32 / 2000) (Train_loss:898.60266113, ACU_loss:1797.20532227, Val_loss:818.01196289)\n",
      "epoch (33 / 2000) (Train_loss:739.74768066, ACU_loss:1479.49536133, Val_loss:677.51092529)\n",
      "epoch (34 / 2000) (Train_loss:587.02957153, ACU_loss:1174.05914307, Val_loss:556.31610107)\n",
      "epoch (35 / 2000) (Train_loss:449.33607483, ACU_loss:898.67214966, Val_loss:432.80044556)\n",
      "epoch (36 / 2000) (Train_loss:324.16267395, ACU_loss:648.32534790, Val_loss:325.99560547)\n",
      "epoch (37 / 2000) (Train_loss:217.71914673, ACU_loss:435.43829346, Val_loss:231.59858704)\n",
      "epoch (38 / 2000) (Train_loss:133.18106461, ACU_loss:266.36212921, Val_loss:147.49378967)\n",
      "epoch (39 / 2000) (Train_loss:73.07559776, ACU_loss:146.15119553, Val_loss:83.36261749)\n",
      "epoch (40 / 2000) (Train_loss:37.72989798, ACU_loss:75.45979595, Val_loss:54.70316696)\n",
      "epoch (41 / 2000) (Train_loss:21.21614969, ACU_loss:42.43229938, Val_loss:51.75438309)\n",
      "epoch (42 / 2000) (Train_loss:20.59281135, ACU_loss:41.18562269, Val_loss:68.10971069)\n",
      "epoch (43 / 2000) (Train_loss:27.93571568, ACU_loss:55.87143135, Val_loss:100.37081146)\n",
      "epoch (44 / 2000) (Train_loss:36.37689877, ACU_loss:72.75379753, Val_loss:151.20674133)\n",
      "epoch (45 / 2000) (Train_loss:40.88358307, ACU_loss:81.76716614, Val_loss:228.90785217)\n",
      "epoch (46 / 2000) (Train_loss:38.36223888, ACU_loss:76.72447777, Val_loss:359.76263428)\n",
      "epoch (47 / 2000) (Train_loss:31.27269983, ACU_loss:62.54539967, Val_loss:433.63955688)\n",
      "epoch (48 / 2000) (Train_loss:22.76793504, ACU_loss:45.53587008, Val_loss:574.47601318)\n",
      "epoch (49 / 2000) (Train_loss:19.54486513, ACU_loss:39.08973026, Val_loss:601.95721436)\n",
      "epoch (50 / 2000) (Train_loss:17.61045265, ACU_loss:35.22090530, Val_loss:467.42495728)\n",
      "epoch (51 / 2000) (Train_loss:16.35989904, ACU_loss:32.71979809, Val_loss:268.64904785)\n",
      "epoch (52 / 2000) (Train_loss:15.49817896, ACU_loss:30.99635792, Val_loss:143.54965210)\n",
      "epoch (53 / 2000) (Train_loss:14.53126347, ACU_loss:29.06252694, Val_loss:90.87874603)\n",
      "epoch (54 / 2000) (Train_loss:13.37281466, ACU_loss:26.74562931, Val_loss:70.52968597)\n",
      "epoch (55 / 2000) (Train_loss:13.11364293, ACU_loss:26.22728586, Val_loss:61.49110794)\n",
      "epoch (56 / 2000) (Train_loss:12.65046179, ACU_loss:25.30092359, Val_loss:56.61809540)\n",
      "epoch (57 / 2000) (Train_loss:11.77557713, ACU_loss:23.55115426, Val_loss:54.04206085)\n",
      "epoch (58 / 2000) (Train_loss:11.31549680, ACU_loss:22.63099360, Val_loss:53.25152206)\n",
      "epoch (59 / 2000) (Train_loss:11.02853593, ACU_loss:22.05707186, Val_loss:54.14430618)\n",
      "epoch (60 / 2000) (Train_loss:10.56202284, ACU_loss:21.12404567, Val_loss:53.63945007)\n",
      "epoch (61 / 2000) (Train_loss:10.30463871, ACU_loss:20.60927743, Val_loss:52.51909637)\n",
      "epoch (62 / 2000) (Train_loss:10.07321543, ACU_loss:20.14643085, Val_loss:50.20522690)\n",
      "epoch (63 / 2000) (Train_loss:9.79317668, ACU_loss:19.58635336, Val_loss:47.34809113)\n",
      "epoch (64 / 2000) (Train_loss:9.48731905, ACU_loss:18.97463810, Val_loss:44.59688187)\n",
      "epoch (65 / 2000) (Train_loss:9.13841301, ACU_loss:18.27682602, Val_loss:43.62638855)\n",
      "epoch (66 / 2000) (Train_loss:8.82651048, ACU_loss:17.65302096, Val_loss:43.74453354)\n",
      "epoch (67 / 2000) (Train_loss:8.60190351, ACU_loss:17.20380703, Val_loss:43.63653183)\n",
      "epoch (68 / 2000) (Train_loss:8.41562917, ACU_loss:16.83125834, Val_loss:43.04953766)\n",
      "epoch (69 / 2000) (Train_loss:8.21662534, ACU_loss:16.43325068, Val_loss:42.01316071)\n",
      "epoch (70 / 2000) (Train_loss:8.01103447, ACU_loss:16.02206895, Val_loss:41.03623199)\n",
      "epoch (71 / 2000) (Train_loss:7.82338092, ACU_loss:15.64676185, Val_loss:40.46830368)\n",
      "epoch (72 / 2000) (Train_loss:7.64498309, ACU_loss:15.28996618, Val_loss:40.07758331)\n",
      "epoch (73 / 2000) (Train_loss:7.47300886, ACU_loss:14.94601771, Val_loss:39.71927643)\n",
      "epoch (74 / 2000) (Train_loss:7.31074121, ACU_loss:14.62148242, Val_loss:39.47529221)\n",
      "epoch (75 / 2000) (Train_loss:7.17268355, ACU_loss:14.34536710, Val_loss:39.19407654)\n",
      "epoch (76 / 2000) (Train_loss:7.03896622, ACU_loss:14.07793243, Val_loss:38.80218887)\n",
      "epoch (77 / 2000) (Train_loss:6.90357735, ACU_loss:13.80715470, Val_loss:38.43808365)\n",
      "epoch (78 / 2000) (Train_loss:6.78410663, ACU_loss:13.56821325, Val_loss:39.36814880)\n",
      "epoch (79 / 2000) (Train_loss:7.41265196, ACU_loss:14.82530391, Val_loss:38.91209030)\n",
      "epoch (80 / 2000) (Train_loss:7.12131153, ACU_loss:14.24262306, Val_loss:38.80188370)\n",
      "epoch (81 / 2000) (Train_loss:7.03267793, ACU_loss:14.06535587, Val_loss:39.01487350)\n",
      "epoch (82 / 2000) (Train_loss:6.89074649, ACU_loss:13.78149298, Val_loss:39.02908325)\n",
      "epoch (83 / 2000) (Train_loss:6.66143821, ACU_loss:13.32287642, Val_loss:38.78758621)\n",
      "epoch (84 / 2000) (Train_loss:6.57843319, ACU_loss:13.15686637, Val_loss:38.43712616)\n",
      "epoch (85 / 2000) (Train_loss:6.48990434, ACU_loss:12.97980868, Val_loss:38.31456757)\n",
      "epoch (86 / 2000) (Train_loss:6.58776821, ACU_loss:13.17553642, Val_loss:38.26934814)\n",
      "epoch (87 / 2000) (Train_loss:6.44052117, ACU_loss:12.88104233, Val_loss:38.64456177)\n",
      "epoch (88 / 2000) (Train_loss:6.28631492, ACU_loss:12.57262984, Val_loss:39.00116730)\n",
      "epoch (89 / 2000) (Train_loss:6.17987758, ACU_loss:12.35975515, Val_loss:38.90501785)\n",
      "epoch (90 / 2000) (Train_loss:6.09697846, ACU_loss:12.19395692, Val_loss:38.32400513)\n",
      "epoch (91 / 2000) (Train_loss:5.99366087, ACU_loss:11.98732175, Val_loss:37.59775925)\n",
      "epoch (92 / 2000) (Train_loss:5.89223494, ACU_loss:11.78446987, Val_loss:37.03235626)\n",
      "epoch (93 / 2000) (Train_loss:5.79560015, ACU_loss:11.59120030, Val_loss:36.86795807)\n",
      "epoch (94 / 2000) (Train_loss:5.69296804, ACU_loss:11.38593609, Val_loss:36.91014481)\n",
      "epoch (95 / 2000) (Train_loss:5.60380293, ACU_loss:11.20760585, Val_loss:36.84830475)\n",
      "epoch (96 / 2000) (Train_loss:5.52973252, ACU_loss:11.05946504, Val_loss:36.59794617)\n",
      "epoch (97 / 2000) (Train_loss:5.44806433, ACU_loss:10.89612867, Val_loss:36.35646820)\n",
      "epoch (98 / 2000) (Train_loss:5.36235550, ACU_loss:10.72471100, Val_loss:36.20884323)\n",
      "epoch (99 / 2000) (Train_loss:5.28041584, ACU_loss:10.56083167, Val_loss:36.12161255)\n",
      "epoch (100 / 2000) (Train_loss:5.20138864, ACU_loss:10.40277728, Val_loss:36.07170868)\n",
      "epoch (101 / 2000) (Train_loss:5.12600012, ACU_loss:10.25200023, Val_loss:35.99196625)\n",
      "epoch (102 / 2000) (Train_loss:5.05272385, ACU_loss:10.10544770, Val_loss:35.90173721)\n",
      "epoch (103 / 2000) (Train_loss:4.98027003, ACU_loss:9.96054007, Val_loss:35.77745819)\n",
      "epoch (104 / 2000) (Train_loss:4.90767532, ACU_loss:9.81535064, Val_loss:35.58312988)\n",
      "epoch (105 / 2000) (Train_loss:4.83658935, ACU_loss:9.67317869, Val_loss:35.49977875)\n",
      "epoch (106 / 2000) (Train_loss:4.76761976, ACU_loss:9.53523953, Val_loss:35.59519577)\n",
      "epoch (107 / 2000) (Train_loss:4.70060758, ACU_loss:9.40121515, Val_loss:35.65975571)\n",
      "epoch (108 / 2000) (Train_loss:4.63860627, ACU_loss:9.27721254, Val_loss:35.62295151)\n",
      "epoch (109 / 2000) (Train_loss:4.57685404, ACU_loss:9.15370807, Val_loss:35.56827545)\n",
      "epoch (110 / 2000) (Train_loss:4.51540856, ACU_loss:9.03081712, Val_loss:35.49810028)\n",
      "epoch (111 / 2000) (Train_loss:4.45612784, ACU_loss:8.91225568, Val_loss:35.41852951)\n",
      "epoch (112 / 2000) (Train_loss:4.39750106, ACU_loss:8.79500211, Val_loss:35.40606689)\n",
      "epoch (113 / 2000) (Train_loss:4.33975292, ACU_loss:8.67950585, Val_loss:35.41267776)\n",
      "epoch (114 / 2000) (Train_loss:4.28309601, ACU_loss:8.56619201, Val_loss:35.40403366)\n",
      "epoch (115 / 2000) (Train_loss:4.22747082, ACU_loss:8.45494165, Val_loss:35.41178894)\n",
      "epoch (116 / 2000) (Train_loss:4.17421270, ACU_loss:8.34842540, Val_loss:35.31641769)\n",
      "epoch (117 / 2000) (Train_loss:4.12129412, ACU_loss:8.24258824, Val_loss:35.39990997)\n",
      "epoch (118 / 2000) (Train_loss:4.06973155, ACU_loss:8.13946311, Val_loss:35.37196732)\n",
      "epoch (119 / 2000) (Train_loss:4.01968034, ACU_loss:8.03936068, Val_loss:35.19535446)\n",
      "epoch (120 / 2000) (Train_loss:3.96956664, ACU_loss:7.93913328, Val_loss:35.15306091)\n",
      "epoch (121 / 2000) (Train_loss:3.92022427, ACU_loss:7.84044854, Val_loss:35.25794220)\n",
      "epoch (122 / 2000) (Train_loss:3.87347110, ACU_loss:7.74694219, Val_loss:35.24210739)\n",
      "epoch (123 / 2000) (Train_loss:3.82603206, ACU_loss:7.65206413, Val_loss:35.23384857)\n",
      "epoch (124 / 2000) (Train_loss:3.77939535, ACU_loss:7.55879070, Val_loss:35.35661316)\n",
      "epoch (125 / 2000) (Train_loss:3.73476978, ACU_loss:7.46953955, Val_loss:35.19853210)\n",
      "epoch (126 / 2000) (Train_loss:3.68777311, ACU_loss:7.37554621, Val_loss:35.29644394)\n",
      "epoch (127 / 2000) (Train_loss:3.64183877, ACU_loss:7.28367755, Val_loss:35.26649094)\n",
      "epoch (128 / 2000) (Train_loss:3.59866872, ACU_loss:7.19733744, Val_loss:35.13593674)\n",
      "epoch (129 / 2000) (Train_loss:3.55559309, ACU_loss:7.11118618, Val_loss:35.31858063)\n",
      "epoch (130 / 2000) (Train_loss:3.51479054, ACU_loss:7.02958108, Val_loss:35.25226593)\n",
      "epoch (131 / 2000) (Train_loss:3.47240648, ACU_loss:6.94481296, Val_loss:35.31827164)\n",
      "epoch (132 / 2000) (Train_loss:3.43146639, ACU_loss:6.86293279, Val_loss:35.40738297)\n",
      "epoch (133 / 2000) (Train_loss:3.39178367, ACU_loss:6.78356733, Val_loss:34.87445831)\n",
      "epoch (134 / 2000) (Train_loss:3.48223376, ACU_loss:6.96446753, Val_loss:34.93901825)\n",
      "epoch (135 / 2000) (Train_loss:3.37765890, ACU_loss:6.75531779, Val_loss:35.16522598)\n",
      "epoch (136 / 2000) (Train_loss:3.35296530, ACU_loss:6.70593061, Val_loss:35.27694321)\n",
      "epoch (137 / 2000) (Train_loss:3.30271521, ACU_loss:6.60543042, Val_loss:35.14211655)\n",
      "epoch (138 / 2000) (Train_loss:3.24197594, ACU_loss:6.48395187, Val_loss:34.85522461)\n",
      "epoch (139 / 2000) (Train_loss:3.18953680, ACU_loss:6.37907359, Val_loss:34.79528046)\n",
      "epoch (140 / 2000) (Train_loss:3.13598053, ACU_loss:6.27196105, Val_loss:35.08406830)\n",
      "epoch (141 / 2000) (Train_loss:3.10431599, ACU_loss:6.20863198, Val_loss:35.23102570)\n",
      "epoch (142 / 2000) (Train_loss:3.06273586, ACU_loss:6.12547172, Val_loss:35.34642410)\n",
      "epoch (143 / 2000) (Train_loss:3.01862678, ACU_loss:6.03725355, Val_loss:35.42136383)\n",
      "epoch (144 / 2000) (Train_loss:2.97795195, ACU_loss:5.95590390, Val_loss:35.26322556)\n",
      "epoch (145 / 2000) (Train_loss:2.93172659, ACU_loss:5.86345317, Val_loss:35.25593185)\n",
      "epoch (146 / 2000) (Train_loss:2.89545111, ACU_loss:5.79090223, Val_loss:35.18906784)\n",
      "epoch (147 / 2000) (Train_loss:2.85724794, ACU_loss:5.71449589, Val_loss:35.11108017)\n",
      "epoch (148 / 2000) (Train_loss:2.81965355, ACU_loss:5.63930711, Val_loss:35.12452698)\n",
      "epoch (149 / 2000) (Train_loss:2.78323711, ACU_loss:5.56647422, Val_loss:35.25863647)\n",
      "epoch (150 / 2000) (Train_loss:2.74491803, ACU_loss:5.48983605, Val_loss:35.43424225)\n",
      "epoch (151 / 2000) (Train_loss:2.71094677, ACU_loss:5.42189355, Val_loss:35.49961472)\n",
      "epoch (152 / 2000) (Train_loss:2.67564673, ACU_loss:5.35129347, Val_loss:35.55813217)\n",
      "epoch (153 / 2000) (Train_loss:2.64066540, ACU_loss:5.28133080, Val_loss:35.48974609)\n",
      "epoch (154 / 2000) (Train_loss:2.60614016, ACU_loss:5.21228032, Val_loss:35.54510117)\n",
      "epoch (155 / 2000) (Train_loss:2.57291181, ACU_loss:5.14582362, Val_loss:36.95047760)\n",
      "epoch (156 / 2000) (Train_loss:2.95902410, ACU_loss:5.91804820, Val_loss:35.11441422)\n",
      "epoch (157 / 2000) (Train_loss:2.68959326, ACU_loss:5.37918653, Val_loss:35.07280731)\n",
      "epoch (158 / 2000) (Train_loss:2.63395309, ACU_loss:5.26790619, Val_loss:36.70941162)\n",
      "epoch (159 / 2000) (Train_loss:2.68275382, ACU_loss:5.36550765, Val_loss:35.03570175)\n",
      "epoch (160 / 2000) (Train_loss:2.48583322, ACU_loss:4.97166644, Val_loss:35.50148010)\n",
      "epoch (161 / 2000) (Train_loss:2.53103827, ACU_loss:5.06207654, Val_loss:37.00611115)\n",
      "epoch (162 / 2000) (Train_loss:2.67237173, ACU_loss:5.34474346, Val_loss:35.63882065)\n",
      "epoch (163 / 2000) (Train_loss:2.44021035, ACU_loss:4.88042070, Val_loss:35.93167496)\n",
      "epoch (164 / 2000) (Train_loss:2.62601922, ACU_loss:5.25203845, Val_loss:34.68038559)\n",
      "epoch (165 / 2000) (Train_loss:2.37343796, ACU_loss:4.74687591, Val_loss:36.76834106)\n",
      "epoch (166 / 2000) (Train_loss:2.46425971, ACU_loss:4.92851941, Val_loss:36.72351456)\n",
      "epoch (167 / 2000) (Train_loss:2.39246490, ACU_loss:4.78492980, Val_loss:35.09749603)\n",
      "epoch (168 / 2000) (Train_loss:2.38906613, ACU_loss:4.77813226, Val_loss:36.15874481)\n",
      "epoch (169 / 2000) (Train_loss:2.33383043, ACU_loss:4.66766086, Val_loss:35.28880692)\n",
      "epoch (170 / 2000) (Train_loss:2.31207880, ACU_loss:4.62415759, Val_loss:36.01589584)\n",
      "epoch (171 / 2000) (Train_loss:2.26757459, ACU_loss:4.53514917, Val_loss:36.50347519)\n",
      "epoch (172 / 2000) (Train_loss:2.16996256, ACU_loss:4.33992511, Val_loss:36.33095169)\n",
      "epoch (173 / 2000) (Train_loss:2.12413326, ACU_loss:4.24826653, Val_loss:35.75534439)\n",
      "epoch (174 / 2000) (Train_loss:2.10257740, ACU_loss:4.20515479, Val_loss:37.31561279)\n",
      "epoch (175 / 2000) (Train_loss:2.33891162, ACU_loss:4.67782323, Val_loss:35.79823685)\n",
      "epoch (176 / 2000) (Train_loss:2.25638814, ACU_loss:4.51277629, Val_loss:35.29054642)\n",
      "epoch (177 / 2000) (Train_loss:2.06182876, ACU_loss:4.12365752, Val_loss:36.78911209)\n",
      "epoch (178 / 2000) (Train_loss:2.44687511, ACU_loss:4.89375022, Val_loss:36.49765396)\n",
      "epoch (179 / 2000) (Train_loss:2.20230424, ACU_loss:4.40460847, Val_loss:35.55947495)\n",
      "epoch (180 / 2000) (Train_loss:2.19244541, ACU_loss:4.38489082, Val_loss:36.61720276)\n",
      "epoch (181 / 2000) (Train_loss:2.10329814, ACU_loss:4.20659627, Val_loss:36.26993942)\n",
      "epoch (182 / 2000) (Train_loss:2.05798171, ACU_loss:4.11596341, Val_loss:35.33237076)\n",
      "epoch (183 / 2000) (Train_loss:2.05523933, ACU_loss:4.11047867, Val_loss:36.31365204)\n",
      "epoch (184 / 2000) (Train_loss:1.95661599, ACU_loss:3.91323198, Val_loss:37.53927231)\n",
      "epoch (185 / 2000) (Train_loss:2.16364786, ACU_loss:4.32729572, Val_loss:35.82277679)\n",
      "epoch (186 / 2000) (Train_loss:1.84627636, ACU_loss:3.69255272, Val_loss:35.29896164)\n",
      "epoch (187 / 2000) (Train_loss:1.88225295, ACU_loss:3.76450591, Val_loss:36.92497253)\n",
      "epoch (188 / 2000) (Train_loss:1.87537054, ACU_loss:3.75074108, Val_loss:36.01094818)\n",
      "epoch (189 / 2000) (Train_loss:1.85271905, ACU_loss:3.70543811, Val_loss:36.12530136)\n",
      "epoch (190 / 2000) (Train_loss:1.82312524, ACU_loss:3.64625047, Val_loss:36.99266052)\n",
      "epoch (191 / 2000) (Train_loss:1.87142573, ACU_loss:3.74285147, Val_loss:35.72071075)\n",
      "epoch (192 / 2000) (Train_loss:1.89803751, ACU_loss:3.79607502, Val_loss:36.81137848)\n",
      "epoch (193 / 2000) (Train_loss:1.76063330, ACU_loss:3.52126659, Val_loss:37.01210022)\n",
      "epoch (194 / 2000) (Train_loss:1.78210895, ACU_loss:3.56421790, Val_loss:35.69602585)\n",
      "epoch (195 / 2000) (Train_loss:1.74839905, ACU_loss:3.49679810, Val_loss:36.15880203)\n",
      "epoch (196 / 2000) (Train_loss:1.77407756, ACU_loss:3.54815513, Val_loss:38.18418503)\n",
      "epoch (197 / 2000) (Train_loss:1.79363595, ACU_loss:3.58727190, Val_loss:37.24492264)\n",
      "epoch (198 / 2000) (Train_loss:1.83289094, ACU_loss:3.66578187, Val_loss:36.16448593)\n",
      "epoch (199 / 2000) (Train_loss:1.66715485, ACU_loss:3.33430970, Val_loss:35.93902206)\n",
      "epoch (200 / 2000) (Train_loss:1.90442163, ACU_loss:3.80884326, Val_loss:37.26702118)\n",
      "epoch (201 / 2000) (Train_loss:1.77712566, ACU_loss:3.55425133, Val_loss:36.44130707)\n",
      "epoch (202 / 2000) (Train_loss:1.69720881, ACU_loss:3.39441761, Val_loss:36.85013199)\n",
      "epoch (203 / 2000) (Train_loss:1.66957901, ACU_loss:3.33915803, Val_loss:35.71556473)\n",
      "epoch (204 / 2000) (Train_loss:1.63546242, ACU_loss:3.27092484, Val_loss:36.04813385)\n",
      "epoch (205 / 2000) (Train_loss:1.58150898, ACU_loss:3.16301797, Val_loss:37.28243637)\n",
      "epoch (206 / 2000) (Train_loss:1.53091371, ACU_loss:3.06182742, Val_loss:37.83134460)\n",
      "epoch (207 / 2000) (Train_loss:1.60402277, ACU_loss:3.20804554, Val_loss:36.93462753)\n",
      "epoch (208 / 2000) (Train_loss:1.49508396, ACU_loss:2.99016792, Val_loss:36.57125473)\n",
      "epoch (209 / 2000) (Train_loss:1.45326346, ACU_loss:2.90652692, Val_loss:37.89734268)\n",
      "epoch (210 / 2000) (Train_loss:1.56451953, ACU_loss:3.12903906, Val_loss:36.27320862)\n",
      "epoch (211 / 2000) (Train_loss:1.46441239, ACU_loss:2.92882477, Val_loss:36.73471451)\n",
      "epoch (212 / 2000) (Train_loss:1.43268525, ACU_loss:2.86537050, Val_loss:37.98534393)\n",
      "epoch (213 / 2000) (Train_loss:1.52743585, ACU_loss:3.05487171, Val_loss:36.91713333)\n"
     ]
    }
   ],
   "source": [
    "# training setting\n",
    "batch=256\n",
    "epoch=2000\n",
    "wafer_len=316\n",
    "dim=16\n",
    "save_path_pattern=''\n",
    "dataset='A456'\n",
    "device='cuda'\n",
    "seed=2\n",
    "comment=''\n",
    "decay=0\n",
    "val_ratio=0.2\n",
    "topk=4\n",
    "report='best'\n",
    "load_model_path=''\n",
    "\n",
    "# set random seed\n",
    "setup_seed(seed)\n",
    "\n",
    "train_config = {\n",
    "    'batch': batch,\n",
    "    'epoch': epoch,\n",
    "    'wafer_len': wafer_len,\n",
    "    'dim': dim,\n",
    "    'comment': comment,\n",
    "    'seed': seed,\n",
    "    'decay': decay,\n",
    "    'val_ratio': val_ratio,\n",
    "    'topk': topk,\n",
    "}\n",
    "\n",
    "env_config={\n",
    "    'save_path': save_path_pattern,\n",
    "    'dataset': dataset,\n",
    "    'report': report,\n",
    "    'device': device,\n",
    "    'load_model_path': load_model_path\n",
    "}\n",
    "\n",
    "# start traning\n",
    "main = Main(train_config, env_config, debug=False)\n",
    "test_loss, _, value_result, _ = main.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A456 MSE :  10.649298429489136\n"
     ]
    }
   ],
   "source": [
    "print(f'{dataset} MSE : ', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch (1 / 2000) (Train_loss:2887.63793945, ACU_loss:5775.27587891, Val_loss:2739.55078125)\n",
      "epoch (2 / 2000) (Train_loss:2838.68359375, ACU_loss:5677.36718750, Val_loss:2721.56201172)\n",
      "epoch (3 / 2000) (Train_loss:2789.79992676, ACU_loss:5579.59985352, Val_loss:2702.57348633)\n",
      "epoch (4 / 2000) (Train_loss:2742.57922363, ACU_loss:5485.15844727, Val_loss:2682.66650391)\n",
      "epoch (5 / 2000) (Train_loss:2696.58056641, ACU_loss:5393.16113281, Val_loss:2660.85156250)\n",
      "epoch (6 / 2000) (Train_loss:2653.74658203, ACU_loss:5307.49316406, Val_loss:2637.70336914)\n",
      "epoch (7 / 2000) (Train_loss:2611.91943359, ACU_loss:5223.83886719, Val_loss:2613.20751953)\n",
      "epoch (8 / 2000) (Train_loss:2564.98156738, ACU_loss:5129.96313477, Val_loss:2586.44287109)\n",
      "epoch (9 / 2000) (Train_loss:2517.25585938, ACU_loss:5034.51171875, Val_loss:2557.11157227)\n",
      "epoch (10 / 2000) (Train_loss:2468.87744141, ACU_loss:4937.75488281, Val_loss:2526.13696289)\n",
      "epoch (11 / 2000) (Train_loss:2420.74414062, ACU_loss:4841.48828125, Val_loss:2494.26635742)\n",
      "epoch (12 / 2000) (Train_loss:2371.32690430, ACU_loss:4742.65380859, Val_loss:2459.69287109)\n",
      "epoch (13 / 2000) (Train_loss:2321.23107910, ACU_loss:4642.46215820, Val_loss:2419.43627930)\n",
      "epoch (14 / 2000) (Train_loss:2272.74255371, ACU_loss:4545.48510742, Val_loss:2373.39770508)\n",
      "epoch (15 / 2000) (Train_loss:2222.88806152, ACU_loss:4445.77612305, Val_loss:2318.85473633)\n",
      "epoch (16 / 2000) (Train_loss:2169.92773438, ACU_loss:4339.85546875, Val_loss:2268.10253906)\n",
      "epoch (17 / 2000) (Train_loss:2126.45812988, ACU_loss:4252.91625977, Val_loss:2204.63940430)\n",
      "epoch (18 / 2000) (Train_loss:2071.78302002, ACU_loss:4143.56604004, Val_loss:2134.76464844)\n",
      "epoch (19 / 2000) (Train_loss:2015.46868896, ACU_loss:4030.93737793, Val_loss:2062.68579102)\n",
      "epoch (20 / 2000) (Train_loss:1959.36193848, ACU_loss:3918.72387695, Val_loss:1991.06726074)\n",
      "epoch (21 / 2000) (Train_loss:1902.82000732, ACU_loss:3805.64001465, Val_loss:1916.01538086)\n",
      "epoch (22 / 2000) (Train_loss:1844.33728027, ACU_loss:3688.67456055, Val_loss:1838.32421875)\n",
      "epoch (23 / 2000) (Train_loss:1782.73535156, ACU_loss:3565.47070312, Val_loss:1769.13183594)\n",
      "epoch (24 / 2000) (Train_loss:1723.20684814, ACU_loss:3446.41369629, Val_loss:1702.92309570)\n",
      "epoch (25 / 2000) (Train_loss:1665.42407227, ACU_loss:3330.84814453, Val_loss:1644.45288086)\n",
      "epoch (26 / 2000) (Train_loss:1610.37927246, ACU_loss:3220.75854492, Val_loss:1589.15295410)\n",
      "epoch (27 / 2000) (Train_loss:1549.96508789, ACU_loss:3099.93017578, Val_loss:1524.77160645)\n",
      "epoch (28 / 2000) (Train_loss:1489.00158691, ACU_loss:2978.00317383, Val_loss:1480.08813477)\n",
      "epoch (29 / 2000) (Train_loss:1426.97448730, ACU_loss:2853.94897461, Val_loss:1431.89758301)\n",
      "epoch (30 / 2000) (Train_loss:1364.93847656, ACU_loss:2729.87695312, Val_loss:1373.64196777)\n",
      "epoch (31 / 2000) (Train_loss:1302.79095459, ACU_loss:2605.58190918, Val_loss:1306.26147461)\n",
      "epoch (32 / 2000) (Train_loss:1240.49627686, ACU_loss:2480.99255371, Val_loss:1239.94238281)\n",
      "epoch (33 / 2000) (Train_loss:1178.42663574, ACU_loss:2356.85327148, Val_loss:1169.48339844)\n",
      "epoch (34 / 2000) (Train_loss:1116.43591309, ACU_loss:2232.87182617, Val_loss:1125.89196777)\n",
      "epoch (35 / 2000) (Train_loss:1072.41900635, ACU_loss:2144.83801270, Val_loss:1056.78991699)\n",
      "epoch (36 / 2000) (Train_loss:1012.64770508, ACU_loss:2025.29541016, Val_loss:1015.62048340)\n",
      "epoch (37 / 2000) (Train_loss:954.48300171, ACU_loss:1908.96600342, Val_loss:971.29125977)\n",
      "epoch (38 / 2000) (Train_loss:896.41558838, ACU_loss:1792.83117676, Val_loss:931.96209717)\n",
      "epoch (39 / 2000) (Train_loss:842.29602051, ACU_loss:1684.59204102, Val_loss:890.54858398)\n",
      "epoch (40 / 2000) (Train_loss:787.90106201, ACU_loss:1575.80212402, Val_loss:832.45074463)\n",
      "epoch (41 / 2000) (Train_loss:737.46780396, ACU_loss:1474.93560791, Val_loss:786.01611328)\n",
      "epoch (42 / 2000) (Train_loss:687.50579834, ACU_loss:1375.01159668, Val_loss:715.92333984)\n",
      "epoch (43 / 2000) (Train_loss:646.91879272, ACU_loss:1293.83758545, Val_loss:670.83831787)\n",
      "epoch (44 / 2000) (Train_loss:595.45098877, ACU_loss:1190.90197754, Val_loss:631.09582520)\n",
      "epoch (45 / 2000) (Train_loss:581.96551514, ACU_loss:1163.93103027, Val_loss:629.69622803)\n",
      "epoch (46 / 2000) (Train_loss:554.61608887, ACU_loss:1109.23217773, Val_loss:564.91168213)\n",
      "epoch (47 / 2000) (Train_loss:504.20272827, ACU_loss:1008.40545654, Val_loss:503.83560181)\n",
      "epoch (48 / 2000) (Train_loss:444.79382324, ACU_loss:889.58764648, Val_loss:454.42196655)\n",
      "epoch (49 / 2000) (Train_loss:397.42910767, ACU_loss:794.85821533, Val_loss:443.35552979)\n",
      "epoch (50 / 2000) (Train_loss:355.81616211, ACU_loss:711.63232422, Val_loss:429.34796143)\n",
      "epoch (51 / 2000) (Train_loss:317.34706116, ACU_loss:634.69412231, Val_loss:398.12161255)\n",
      "epoch (52 / 2000) (Train_loss:292.72572327, ACU_loss:585.45144653, Val_loss:341.03659058)\n",
      "epoch (53 / 2000) (Train_loss:264.68927002, ACU_loss:529.37854004, Val_loss:273.92492676)\n",
      "epoch (54 / 2000) (Train_loss:235.00984955, ACU_loss:470.01969910, Val_loss:230.29531860)\n",
      "epoch (55 / 2000) (Train_loss:205.68795013, ACU_loss:411.37590027, Val_loss:192.50575256)\n",
      "epoch (56 / 2000) (Train_loss:187.65740967, ACU_loss:375.31481934, Val_loss:171.55787659)\n",
      "epoch (57 / 2000) (Train_loss:162.55959320, ACU_loss:325.11918640, Val_loss:162.31912231)\n",
      "epoch (58 / 2000) (Train_loss:145.28345871, ACU_loss:290.56691742, Val_loss:153.64123535)\n",
      "epoch (59 / 2000) (Train_loss:125.01610565, ACU_loss:250.03221130, Val_loss:143.36254883)\n",
      "epoch (60 / 2000) (Train_loss:106.85166168, ACU_loss:213.70332336, Val_loss:129.71224976)\n",
      "epoch (61 / 2000) (Train_loss:92.04017639, ACU_loss:184.08035278, Val_loss:101.26130676)\n",
      "epoch (62 / 2000) (Train_loss:74.90547180, ACU_loss:149.81094360, Val_loss:88.10960388)\n",
      "epoch (63 / 2000) (Train_loss:63.99731064, ACU_loss:127.99462128, Val_loss:75.75569153)\n",
      "epoch (64 / 2000) (Train_loss:54.94919968, ACU_loss:109.89839935, Val_loss:60.60179138)\n",
      "epoch (65 / 2000) (Train_loss:47.47598171, ACU_loss:94.95196342, Val_loss:55.65546417)\n",
      "epoch (66 / 2000) (Train_loss:42.57022095, ACU_loss:85.14044189, Val_loss:48.90451813)\n",
      "epoch (67 / 2000) (Train_loss:37.40096951, ACU_loss:74.80193901, Val_loss:45.58985901)\n",
      "epoch (68 / 2000) (Train_loss:33.23339987, ACU_loss:66.46679974, Val_loss:42.78734970)\n",
      "epoch (69 / 2000) (Train_loss:29.95646954, ACU_loss:59.91293907, Val_loss:47.26230240)\n",
      "epoch (70 / 2000) (Train_loss:28.85783625, ACU_loss:57.71567249, Val_loss:41.91983032)\n",
      "epoch (71 / 2000) (Train_loss:26.69594288, ACU_loss:53.39188576, Val_loss:38.61739349)\n",
      "epoch (72 / 2000) (Train_loss:24.99584341, ACU_loss:49.99168682, Val_loss:37.59791946)\n",
      "epoch (73 / 2000) (Train_loss:23.66804790, ACU_loss:47.33609581, Val_loss:37.53678513)\n",
      "epoch (74 / 2000) (Train_loss:22.77174425, ACU_loss:45.54348850, Val_loss:38.28915405)\n",
      "epoch (75 / 2000) (Train_loss:21.88581347, ACU_loss:43.77162695, Val_loss:38.64968491)\n",
      "epoch (76 / 2000) (Train_loss:21.14324188, ACU_loss:42.28648376, Val_loss:38.54647827)\n",
      "epoch (77 / 2000) (Train_loss:20.49332905, ACU_loss:40.98665810, Val_loss:38.17896652)\n",
      "epoch (78 / 2000) (Train_loss:19.92040539, ACU_loss:39.84081078, Val_loss:37.87326050)\n",
      "epoch (79 / 2000) (Train_loss:19.39386749, ACU_loss:38.78773499, Val_loss:37.17605209)\n",
      "epoch (80 / 2000) (Train_loss:18.76192808, ACU_loss:37.52385616, Val_loss:39.73079681)\n",
      "epoch (81 / 2000) (Train_loss:18.26786113, ACU_loss:36.53572226, Val_loss:38.91370773)\n",
      "epoch (82 / 2000) (Train_loss:17.78529215, ACU_loss:35.57058430, Val_loss:37.85633850)\n",
      "epoch (83 / 2000) (Train_loss:17.33512378, ACU_loss:34.67024755, Val_loss:36.56971359)\n",
      "epoch (84 / 2000) (Train_loss:16.91249883, ACU_loss:33.82499766, Val_loss:35.28330994)\n",
      "epoch (85 / 2000) (Train_loss:16.52491999, ACU_loss:33.04983997, Val_loss:34.26107025)\n",
      "epoch (86 / 2000) (Train_loss:16.20309615, ACU_loss:32.40619230, Val_loss:33.50987244)\n",
      "epoch (87 / 2000) (Train_loss:15.90710425, ACU_loss:31.81420851, Val_loss:32.96793747)\n",
      "epoch (88 / 2000) (Train_loss:15.61440551, ACU_loss:31.22881103, Val_loss:31.58996391)\n",
      "epoch (89 / 2000) (Train_loss:16.02301109, ACU_loss:32.04602218, Val_loss:31.33013153)\n",
      "epoch (90 / 2000) (Train_loss:15.91216123, ACU_loss:31.82432246, Val_loss:31.19284248)\n",
      "epoch (91 / 2000) (Train_loss:15.58237863, ACU_loss:31.16475725, Val_loss:31.20587158)\n",
      "epoch (92 / 2000) (Train_loss:15.27065217, ACU_loss:30.54130435, Val_loss:31.15288544)\n",
      "epoch (93 / 2000) (Train_loss:14.98427916, ACU_loss:29.96855831, Val_loss:30.86364937)\n",
      "epoch (94 / 2000) (Train_loss:14.71456718, ACU_loss:29.42913437, Val_loss:30.32796288)\n",
      "epoch (95 / 2000) (Train_loss:14.45085716, ACU_loss:28.90171432, Val_loss:30.06657600)\n",
      "epoch (96 / 2000) (Train_loss:14.20813578, ACU_loss:28.41627157, Val_loss:29.66793251)\n",
      "epoch (97 / 2000) (Train_loss:13.97248375, ACU_loss:27.94496751, Val_loss:29.39339066)\n",
      "epoch (98 / 2000) (Train_loss:13.74580908, ACU_loss:27.49161816, Val_loss:29.19464874)\n",
      "epoch (99 / 2000) (Train_loss:13.52735794, ACU_loss:27.05471587, Val_loss:29.02823257)\n",
      "epoch (100 / 2000) (Train_loss:13.31453562, ACU_loss:26.62907124, Val_loss:28.86765289)\n",
      "epoch (101 / 2000) (Train_loss:13.10825861, ACU_loss:26.21651721, Val_loss:28.71025848)\n",
      "epoch (102 / 2000) (Train_loss:12.90881556, ACU_loss:25.81763113, Val_loss:28.57201576)\n",
      "epoch (103 / 2000) (Train_loss:12.88854778, ACU_loss:25.77709556, Val_loss:28.66415215)\n",
      "epoch (104 / 2000) (Train_loss:12.74997824, ACU_loss:25.49995649, Val_loss:28.58491707)\n",
      "epoch (105 / 2000) (Train_loss:12.49883360, ACU_loss:24.99766719, Val_loss:28.56056976)\n",
      "epoch (106 / 2000) (Train_loss:12.28189296, ACU_loss:24.56378591, Val_loss:28.53237724)\n",
      "epoch (107 / 2000) (Train_loss:12.11436355, ACU_loss:24.22872710, Val_loss:28.45808029)\n",
      "epoch (108 / 2000) (Train_loss:11.96051133, ACU_loss:23.92102265, Val_loss:28.35067940)\n",
      "epoch (109 / 2000) (Train_loss:11.79079074, ACU_loss:23.58158147, Val_loss:28.27038383)\n",
      "epoch (110 / 2000) (Train_loss:11.61095345, ACU_loss:23.22190690, Val_loss:28.26718521)\n",
      "epoch (111 / 2000) (Train_loss:11.44191241, ACU_loss:22.88382483, Val_loss:28.33926201)\n",
      "epoch (112 / 2000) (Train_loss:11.28982052, ACU_loss:22.57964104, Val_loss:28.40319061)\n",
      "epoch (113 / 2000) (Train_loss:11.14746413, ACU_loss:22.29492825, Val_loss:28.38279915)\n",
      "epoch (114 / 2000) (Train_loss:11.00230199, ACU_loss:22.00460398, Val_loss:28.23380089)\n",
      "epoch (115 / 2000) (Train_loss:10.85730278, ACU_loss:21.71460557, Val_loss:27.99285316)\n",
      "epoch (116 / 2000) (Train_loss:10.72072190, ACU_loss:21.44144380, Val_loss:27.75936699)\n",
      "epoch (117 / 2000) (Train_loss:10.59210819, ACU_loss:21.18421638, Val_loss:27.59371948)\n",
      "epoch (118 / 2000) (Train_loss:10.46938166, ACU_loss:20.93876332, Val_loss:27.50227165)\n",
      "epoch (119 / 2000) (Train_loss:10.34739637, ACU_loss:20.69479275, Val_loss:27.46267509)\n",
      "epoch (120 / 2000) (Train_loss:10.22452351, ACU_loss:20.44904703, Val_loss:27.48921967)\n",
      "epoch (121 / 2000) (Train_loss:10.10537219, ACU_loss:20.21074438, Val_loss:27.53716660)\n",
      "epoch (122 / 2000) (Train_loss:9.98980558, ACU_loss:19.97961116, Val_loss:27.55412483)\n",
      "epoch (123 / 2000) (Train_loss:9.87787348, ACU_loss:19.75574696, Val_loss:27.47516251)\n",
      "epoch (124 / 2000) (Train_loss:9.76658285, ACU_loss:19.53316569, Val_loss:27.35052299)\n",
      "epoch (125 / 2000) (Train_loss:9.65774548, ACU_loss:19.31549096, Val_loss:27.24318123)\n",
      "epoch (126 / 2000) (Train_loss:9.55020738, ACU_loss:19.10041475, Val_loss:27.19375992)\n",
      "epoch (127 / 2000) (Train_loss:9.44608119, ACU_loss:18.89216238, Val_loss:27.20968628)\n",
      "epoch (128 / 2000) (Train_loss:9.34539747, ACU_loss:18.69079494, Val_loss:27.26332283)\n",
      "epoch (129 / 2000) (Train_loss:9.24575442, ACU_loss:18.49150884, Val_loss:27.33739662)\n",
      "epoch (130 / 2000) (Train_loss:9.14813066, ACU_loss:18.29626131, Val_loss:27.38580894)\n",
      "epoch (131 / 2000) (Train_loss:9.05247402, ACU_loss:18.10494804, Val_loss:27.38381767)\n",
      "epoch (132 / 2000) (Train_loss:8.95843315, ACU_loss:17.91686630, Val_loss:27.34132767)\n",
      "epoch (133 / 2000) (Train_loss:8.86630258, ACU_loss:17.73260516, Val_loss:27.28942108)\n",
      "epoch (134 / 2000) (Train_loss:8.77465633, ACU_loss:17.54931265, Val_loss:27.22292328)\n",
      "epoch (135 / 2000) (Train_loss:8.68530081, ACU_loss:17.37060162, Val_loss:27.14305305)\n",
      "epoch (136 / 2000) (Train_loss:8.59725629, ACU_loss:17.19451258, Val_loss:27.06680870)\n",
      "epoch (137 / 2000) (Train_loss:8.51081106, ACU_loss:17.02162212, Val_loss:27.00279999)\n",
      "epoch (138 / 2000) (Train_loss:8.42563407, ACU_loss:16.85126814, Val_loss:26.99051476)\n",
      "epoch (139 / 2000) (Train_loss:8.34193596, ACU_loss:16.68387192, Val_loss:27.02099037)\n",
      "epoch (140 / 2000) (Train_loss:8.25966558, ACU_loss:16.51933116, Val_loss:27.06795883)\n",
      "epoch (141 / 2000) (Train_loss:8.17880444, ACU_loss:16.35760888, Val_loss:27.09646034)\n",
      "epoch (142 / 2000) (Train_loss:8.09943132, ACU_loss:16.19886264, Val_loss:27.10254669)\n",
      "epoch (143 / 2000) (Train_loss:8.02042109, ACU_loss:16.04084218, Val_loss:27.10492706)\n",
      "epoch (144 / 2000) (Train_loss:7.94266832, ACU_loss:15.88533664, Val_loss:27.10624886)\n",
      "epoch (145 / 2000) (Train_loss:7.86588150, ACU_loss:15.73176301, Val_loss:27.08429527)\n",
      "epoch (146 / 2000) (Train_loss:7.78989808, ACU_loss:15.57979617, Val_loss:27.04218292)\n",
      "epoch (147 / 2000) (Train_loss:7.71420702, ACU_loss:15.42841405, Val_loss:27.02586555)\n",
      "epoch (148 / 2000) (Train_loss:7.63876174, ACU_loss:15.27752349, Val_loss:27.05661964)\n",
      "epoch (149 / 2000) (Train_loss:7.56354277, ACU_loss:15.12708554, Val_loss:27.06237221)\n",
      "epoch (150 / 2000) (Train_loss:7.48987556, ACU_loss:14.97975111, Val_loss:27.03179550)\n",
      "epoch (151 / 2000) (Train_loss:7.41601829, ACU_loss:14.83203658, Val_loss:26.97891617)\n",
      "epoch (152 / 2000) (Train_loss:7.34196383, ACU_loss:14.68392766, Val_loss:26.94987297)\n",
      "epoch (153 / 2000) (Train_loss:7.26866291, ACU_loss:14.53732583, Val_loss:26.95858574)\n",
      "epoch (154 / 2000) (Train_loss:7.19707452, ACU_loss:14.39414904, Val_loss:27.00025177)\n",
      "epoch (155 / 2000) (Train_loss:7.12591001, ACU_loss:14.25182003, Val_loss:27.06085205)\n",
      "epoch (156 / 2000) (Train_loss:7.05491158, ACU_loss:14.10982317, Val_loss:27.14469528)\n",
      "epoch (157 / 2000) (Train_loss:6.98515028, ACU_loss:13.97030056, Val_loss:27.16024208)\n",
      "epoch (158 / 2000) (Train_loss:6.91642255, ACU_loss:13.83284509, Val_loss:27.13714790)\n",
      "epoch (159 / 2000) (Train_loss:6.84867850, ACU_loss:13.69735700, Val_loss:27.09051323)\n",
      "epoch (160 / 2000) (Train_loss:6.78140607, ACU_loss:13.56281215, Val_loss:27.04965973)\n",
      "epoch (161 / 2000) (Train_loss:6.71427897, ACU_loss:13.42855793, Val_loss:27.02104568)\n",
      "epoch (162 / 2000) (Train_loss:6.64693642, ACU_loss:13.29387283, Val_loss:27.01439285)\n",
      "epoch (163 / 2000) (Train_loss:6.57966046, ACU_loss:13.15932092, Val_loss:27.03690529)\n",
      "epoch (164 / 2000) (Train_loss:6.52389626, ACU_loss:13.04779252, Val_loss:27.02856064)\n",
      "epoch (165 / 2000) (Train_loss:6.46559229, ACU_loss:12.93118459, Val_loss:27.03315163)\n",
      "epoch (166 / 2000) (Train_loss:6.39858577, ACU_loss:12.79717153, Val_loss:27.03738976)\n",
      "epoch (167 / 2000) (Train_loss:6.33227745, ACU_loss:12.66455489, Val_loss:26.97311592)\n",
      "epoch (168 / 2000) (Train_loss:6.26609445, ACU_loss:12.53218889, Val_loss:26.87291527)\n",
      "epoch (169 / 2000) (Train_loss:6.20037156, ACU_loss:12.40074313, Val_loss:26.83921051)\n",
      "epoch (170 / 2000) (Train_loss:6.13550118, ACU_loss:12.27100235, Val_loss:26.86064148)\n",
      "epoch (171 / 2000) (Train_loss:6.07073514, ACU_loss:12.14147028, Val_loss:26.89073181)\n",
      "epoch (172 / 2000) (Train_loss:6.00658782, ACU_loss:12.01317564, Val_loss:26.86872292)\n",
      "epoch (173 / 2000) (Train_loss:5.94322518, ACU_loss:11.88645035, Val_loss:26.79936028)\n",
      "epoch (174 / 2000) (Train_loss:5.87972485, ACU_loss:11.75944969, Val_loss:26.76643181)\n",
      "epoch (175 / 2000) (Train_loss:5.81646121, ACU_loss:11.63292241, Val_loss:26.79681778)\n",
      "epoch (176 / 2000) (Train_loss:5.75309676, ACU_loss:11.50619352, Val_loss:26.86077499)\n",
      "epoch (177 / 2000) (Train_loss:5.69098431, ACU_loss:11.38196862, Val_loss:26.85504150)\n",
      "epoch (178 / 2000) (Train_loss:5.62909453, ACU_loss:11.25818907, Val_loss:26.82998276)\n",
      "epoch (179 / 2000) (Train_loss:5.56669828, ACU_loss:11.13339657, Val_loss:26.86011505)\n",
      "epoch (180 / 2000) (Train_loss:5.50435029, ACU_loss:11.00870058, Val_loss:26.92881966)\n",
      "epoch (181 / 2000) (Train_loss:5.44297633, ACU_loss:10.88595265, Val_loss:26.95487785)\n",
      "epoch (182 / 2000) (Train_loss:5.38223408, ACU_loss:10.76446816, Val_loss:27.17684937)\n",
      "epoch (183 / 2000) (Train_loss:5.32108240, ACU_loss:10.64216480, Val_loss:26.89469337)\n",
      "epoch (184 / 2000) (Train_loss:5.21848875, ACU_loss:10.43697749, Val_loss:26.69241905)\n",
      "epoch (185 / 2000) (Train_loss:5.16326618, ACU_loss:10.32653236, Val_loss:26.67206573)\n",
      "epoch (186 / 2000) (Train_loss:5.11245873, ACU_loss:10.22491746, Val_loss:26.78457642)\n",
      "epoch (187 / 2000) (Train_loss:5.03113711, ACU_loss:10.06227422, Val_loss:26.93725014)\n",
      "epoch (188 / 2000) (Train_loss:4.96900281, ACU_loss:9.93800561, Val_loss:26.95506477)\n",
      "epoch (189 / 2000) (Train_loss:4.91937631, ACU_loss:9.83875263, Val_loss:26.83730316)\n",
      "epoch (190 / 2000) (Train_loss:5.14530477, ACU_loss:10.29060954, Val_loss:27.46736526)\n",
      "epoch (191 / 2000) (Train_loss:5.48982137, ACU_loss:10.97964275, Val_loss:26.47987747)\n",
      "epoch (192 / 2000) (Train_loss:5.30045328, ACU_loss:10.60090655, Val_loss:26.04512215)\n",
      "epoch (193 / 2000) (Train_loss:5.16053104, ACU_loss:10.32106209, Val_loss:25.90279579)\n",
      "epoch (194 / 2000) (Train_loss:5.01261014, ACU_loss:10.02522027, Val_loss:26.47554970)\n",
      "epoch (195 / 2000) (Train_loss:5.01600647, ACU_loss:10.03201294, Val_loss:26.57060432)\n",
      "epoch (196 / 2000) (Train_loss:4.73694369, ACU_loss:9.47388738, Val_loss:28.26519203)\n",
      "epoch (197 / 2000) (Train_loss:4.93137193, ACU_loss:9.86274385, Val_loss:26.59747505)\n",
      "epoch (198 / 2000) (Train_loss:4.75266269, ACU_loss:9.50532538, Val_loss:25.41335106)\n",
      "epoch (199 / 2000) (Train_loss:4.68693314, ACU_loss:9.37386629, Val_loss:25.26161575)\n",
      "epoch (200 / 2000) (Train_loss:4.63215111, ACU_loss:9.26430222, Val_loss:25.40483856)\n",
      "epoch (201 / 2000) (Train_loss:4.51803889, ACU_loss:9.03607778, Val_loss:25.93865395)\n",
      "epoch (202 / 2000) (Train_loss:4.47225072, ACU_loss:8.94450144, Val_loss:26.03996849)\n",
      "epoch (203 / 2000) (Train_loss:4.40513068, ACU_loss:8.81026137, Val_loss:25.68545151)\n",
      "epoch (204 / 2000) (Train_loss:4.31285634, ACU_loss:8.62571268, Val_loss:25.65764427)\n",
      "epoch (205 / 2000) (Train_loss:4.26890813, ACU_loss:8.53781626, Val_loss:25.86004448)\n",
      "epoch (206 / 2000) (Train_loss:4.19797140, ACU_loss:8.39594281, Val_loss:26.17274094)\n",
      "epoch (207 / 2000) (Train_loss:4.13166331, ACU_loss:8.26332662, Val_loss:26.12088966)\n",
      "epoch (208 / 2000) (Train_loss:4.09416024, ACU_loss:8.18832047, Val_loss:25.84247398)\n",
      "epoch (209 / 2000) (Train_loss:4.36949736, ACU_loss:8.73899472, Val_loss:26.69984818)\n",
      "epoch (210 / 2000) (Train_loss:3.98771869, ACU_loss:7.97543737, Val_loss:26.37673378)\n",
      "epoch (211 / 2000) (Train_loss:3.95699348, ACU_loss:7.91398697, Val_loss:26.39435959)\n",
      "epoch (212 / 2000) (Train_loss:3.96989038, ACU_loss:7.93978076, Val_loss:26.19041252)\n",
      "epoch (213 / 2000) (Train_loss:3.89991072, ACU_loss:7.79982144, Val_loss:26.17230988)\n",
      "epoch (214 / 2000) (Train_loss:3.83424911, ACU_loss:7.66849822, Val_loss:26.38546753)\n",
      "epoch (215 / 2000) (Train_loss:3.76707792, ACU_loss:7.53415584, Val_loss:26.59028625)\n",
      "epoch (216 / 2000) (Train_loss:3.70663274, ACU_loss:7.41326548, Val_loss:26.66689301)\n",
      "epoch (217 / 2000) (Train_loss:3.65927900, ACU_loss:7.31855801, Val_loss:26.70529175)\n",
      "epoch (218 / 2000) (Train_loss:3.61639526, ACU_loss:7.23279053, Val_loss:26.88353920)\n",
      "epoch (219 / 2000) (Train_loss:3.56252063, ACU_loss:7.12504127, Val_loss:27.15378952)\n",
      "epoch (220 / 2000) (Train_loss:3.58844569, ACU_loss:7.17689137, Val_loss:27.92090607)\n",
      "epoch (221 / 2000) (Train_loss:3.48066569, ACU_loss:6.96133139, Val_loss:27.23236847)\n",
      "epoch (222 / 2000) (Train_loss:3.43290420, ACU_loss:6.86580840, Val_loss:26.69314957)\n",
      "epoch (223 / 2000) (Train_loss:3.38458501, ACU_loss:6.76917002, Val_loss:26.76337433)\n",
      "epoch (224 / 2000) (Train_loss:3.33403745, ACU_loss:6.66807491, Val_loss:26.86506462)\n",
      "epoch (225 / 2000) (Train_loss:3.28168140, ACU_loss:6.56336281, Val_loss:26.84134102)\n",
      "epoch (226 / 2000) (Train_loss:3.54455251, ACU_loss:7.08910501, Val_loss:26.80259895)\n",
      "epoch (227 / 2000) (Train_loss:3.22326893, ACU_loss:6.44653785, Val_loss:27.20259285)\n",
      "epoch (228 / 2000) (Train_loss:3.23098489, ACU_loss:6.46196978, Val_loss:27.27994347)\n"
     ]
    }
   ],
   "source": [
    "# training setting\n",
    "batch=256\n",
    "epoch=2000\n",
    "wafer_len=316\n",
    "dim=16\n",
    "save_path_pattern=''\n",
    "dataset='B456'\n",
    "device='cuda'\n",
    "seed=2\n",
    "comment=''\n",
    "decay=0\n",
    "val_ratio=0.2\n",
    "topk=4\n",
    "report='best'\n",
    "load_model_path=''\n",
    "\n",
    "# set random seed\n",
    "setup_seed(seed)\n",
    "\n",
    "train_config = {\n",
    "    'batch': batch,\n",
    "    'epoch': epoch,\n",
    "    'wafer_len': wafer_len,\n",
    "    'dim': dim,\n",
    "    'comment': comment,\n",
    "    'seed': seed,\n",
    "    'decay': decay,\n",
    "    'val_ratio': val_ratio,\n",
    "    'topk': topk,\n",
    "}\n",
    "\n",
    "env_config={\n",
    "    'save_path': save_path_pattern,\n",
    "    'dataset': dataset,\n",
    "    'report': report,\n",
    "    'device': device,\n",
    "    'load_model_path': load_model_path\n",
    "}\n",
    "\n",
    "# start traning\n",
    "main = Main(train_config, env_config, debug=False)\n",
    "test_loss, _, value_result, _ = main.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B456 MSE :  11.936854362487793\n"
     ]
    }
   ],
   "source": [
    "print(f'{dataset} MSE : ', test_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wuenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
