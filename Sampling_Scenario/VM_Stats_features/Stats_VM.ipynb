{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Features VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import kurtosis, skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the latest MRR column\n",
    "def get_orig_data(dataset):\n",
    "    \n",
    "    train_orig = pd.read_csv(f'./data/{dataset}/train.csv', sep=',', index_col=0).reset_index(drop=True)\n",
    "    test_orig = pd.read_csv(f'./data/{dataset}/test.csv', sep=',', index_col=0).reset_index(drop=True)\n",
    "\n",
    "    short_sampled_orig = train_orig.iloc[316*2:, :].reset_index(drop=True)\n",
    "    latest_mrr_sampled = train_orig.iloc[:-316*2, -1].rename('latest_mrr').reset_index(drop=True)\n",
    "    addmrr_sampled_orig = pd.concat([short_sampled_orig, latest_mrr_sampled], axis=1)\n",
    "\n",
    "    short_unsampled_orig = train_orig.iloc[316*1:, :].reset_index(drop=True)\n",
    "    latest_mrr_unsampled = train_orig.iloc[:-316*1, -1].rename('latest_mrr').reset_index(drop=True)\n",
    "    addmrr_unsampled_orig = pd.concat([short_unsampled_orig, latest_mrr_unsampled], axis=1)\n",
    "\n",
    "    latest_mrr_test = pd.concat([train_orig.iloc[-316*1:, -1], test_orig.iloc[:-316*1, -1]], ignore_index=True).rename(\"latest_mrr\").reset_index(drop=True)\n",
    "    addmrr_test_orig = pd.concat([test_orig, latest_mrr_test], axis=1)\n",
    "    \n",
    "    return addmrr_sampled_orig, addmrr_unsampled_orig, addmrr_test_orig\n",
    "\n",
    "# transfer into the input data\n",
    "def get_stats_features(data):\n",
    "    orig_X = data.iloc[:, :-2].to_numpy()\n",
    "    orig_X = orig_X.reshape(-1, 316, orig_X.shape[1])\n",
    "    orig_y = data.iloc[:, -2].tolist()\n",
    "    data_y = [orig_y[i] for i in range(0, len(orig_y), 316)]\n",
    "    # create the latest MRR for every samples\n",
    "    latest_mrr = data.iloc[:, -1].tolist()\n",
    "    latest_mrr = [latest_mrr[i] for i in range(0, len(latest_mrr), 316)]\n",
    "    \n",
    "    # calculate the statistics features\n",
    "    means = np.mean(orig_X, axis=1)\n",
    "    stds = np.std(orig_X, axis=1)\n",
    "    medians = np.median(orig_X, axis=1)\n",
    "    mins = np.min(orig_X, axis=1)\n",
    "    maxs = np.max(orig_X, axis=1)\n",
    "    kurts = kurtosis(orig_X, axis=1)\n",
    "    skews = skew(orig_X, axis=1)\n",
    "    stats_X = np.hstack([means, stds, medians, mins, maxs, kurts, skews])\n",
    "    stats_X = np.nan_to_num(stats_X, nan=0.0)\n",
    "\n",
    "    # add the latest MRR\n",
    "    latest_mrr = np.array(latest_mrr).reshape(-1,1)\n",
    "    data_X = np.concatenate((stats_X, latest_mrr), axis=1)\n",
    "\n",
    "    return data_X, data_y\n",
    "\n",
    "# fit different ML models\n",
    "def fit_XGB(X_train, y_train):\n",
    "    import xgboost as xgb\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',  # 回歸問題\n",
    "        'max_depth': 3,                    # 樹的最大深度\n",
    "        'learning_rate': 0.01,              # 學習率\n",
    "        'n_estimators': 200                # 樹的數量\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def fit_RF(X_train, y_train):\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=2)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def fit_KNN(X_train, y_train):\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    model = KNeighborsRegressor(n_neighbors=5)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def fit_MLP(X_train, y_train):\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    model = MLPRegressor(hidden_layer_sizes=(128, 16), activation='relu', solver='adam', max_iter=2000, random_state=2)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "# generate the VM MRR prediction\n",
    "def get_VM_predition(dataset, model_type):  # model_type: 'XGB'/'RF'/'KNN'/\"MLP\"\n",
    "    \n",
    "    addmrr_sampled_orig, addmrr_unsampled_orig, addmrr_test_orig = get_orig_data(dataset)\n",
    "    \n",
    "    # retain only sampled section for training\n",
    "    sampled_train_orig = pd.concat([addmrr_sampled_orig.iloc[i:i+316] for i in range(0, len(addmrr_sampled_orig), 316*2)], ignore_index=True)\n",
    "    unsampled_train_orig = pd.concat([addmrr_unsampled_orig.iloc[i:i+316] for i in range(316, len(addmrr_unsampled_orig), 316*2)], ignore_index=True)\n",
    "    all_test_orig = pd.concat([unsampled_train_orig, addmrr_test_orig], ignore_index=True)\n",
    "    extend_test_orig = pd.concat([addmrr_unsampled_orig.iloc[-316*4:], addmrr_test_orig], ignore_index=True)\n",
    "\n",
    "    y_pred = {}\n",
    "    X_train, y_train = get_stats_features(sampled_train_orig)\n",
    "    # unsampled : unsampled wafer in training set / test : testing set / all : 'unsampled and test' / extend : 'testing set extended to past wafers'(for equipment state model)\n",
    "    data_modes = {'unsampled':unsampled_train_orig, 'test':addmrr_test_orig, 'all':all_test_orig, 'extend':extend_test_orig}\n",
    "    for mode, data in data_modes.items():\n",
    "        match model_type:\n",
    "            case 'XGB':\n",
    "                model = fit_XGB(X_train, y_train)\n",
    "            case 'RF':\n",
    "                model = fit_RF(X_train, y_train)\n",
    "            case 'KNN':\n",
    "                model = fit_KNN(X_train, y_train)\n",
    "            case 'MLP':\n",
    "                model = fit_MLP(X_train, y_train)\n",
    "\n",
    "        X_test, y_test = get_stats_features(data)\n",
    "        pred = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, pred)\n",
    "\n",
    "        print('------------------------------------------------')\n",
    "        print(mode)\n",
    "        print(f\"Statistical Features + {model_type} VM test loss\", round(mse, 3))\n",
    "        y_pred[mode] = model.predict(X_test)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "unsampled\n",
      "Statistical Features + XGB VM test loss 8.82\n",
      "------------------------------------------------\n",
      "test\n",
      "Statistical Features + XGB VM test loss 14.96\n",
      "------------------------------------------------\n",
      "all\n",
      "Statistical Features + XGB VM test loss 11.659\n",
      "------------------------------------------------\n",
      "extend\n",
      "Statistical Features + XGB VM test loss 15.013\n"
     ]
    }
   ],
   "source": [
    "# XGB\n",
    "dataset = 'A456'\n",
    "model_type = 'XGB'\n",
    "vm_pred = get_VM_predition(dataset, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "unsampled\n",
      "Statistical Features + RF VM test loss 4.381\n",
      "------------------------------------------------\n",
      "test\n",
      "Statistical Features + RF VM test loss 12.192\n",
      "------------------------------------------------\n",
      "all\n",
      "Statistical Features + RF VM test loss 7.993\n",
      "------------------------------------------------\n",
      "extend\n",
      "Statistical Features + RF VM test loss 12.179\n"
     ]
    }
   ],
   "source": [
    "# RF\n",
    "dataset = 'A456'\n",
    "model_type = 'RF'\n",
    "vm_pred = get_VM_predition(dataset, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "unsampled\n",
      "Statistical Features + KNN VM test loss 9.44\n",
      "------------------------------------------------\n",
      "test\n",
      "Statistical Features + KNN VM test loss 17.586\n",
      "------------------------------------------------\n",
      "all\n",
      "Statistical Features + KNN VM test loss 13.207\n",
      "------------------------------------------------\n",
      "extend\n",
      "Statistical Features + KNN VM test loss 17.657\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "dataset = 'A456'\n",
    "model_type = 'KNN'\n",
    "vm_pred = get_VM_predition(dataset, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "unsampled\n",
      "Statistical Features + MLP VM test loss 7.886\n",
      "------------------------------------------------\n",
      "test\n",
      "Statistical Features + MLP VM test loss 18.699\n",
      "------------------------------------------------\n",
      "all\n",
      "Statistical Features + MLP VM test loss 12.886\n",
      "------------------------------------------------\n",
      "extend\n",
      "Statistical Features + MLP VM test loss 18.517\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "dataset = 'A456'\n",
    "model_type = 'MLP'\n",
    "vm_pred = get_VM_predition(dataset, model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "unsampled\n",
      "Statistical Features + XGB VM test loss 12.319\n",
      "------------------------------------------------\n",
      "test\n",
      "Statistical Features + XGB VM test loss 25.443\n",
      "------------------------------------------------\n",
      "all\n",
      "Statistical Features + XGB VM test loss 18.396\n",
      "------------------------------------------------\n",
      "extend\n",
      "Statistical Features + XGB VM test loss 25.273\n"
     ]
    }
   ],
   "source": [
    "# XGB\n",
    "dataset = 'B456'\n",
    "model_type = 'XGB'\n",
    "vm_pred = get_VM_predition(dataset, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "unsampled\n",
      "Statistical Features + RF VM test loss 5.976\n",
      "------------------------------------------------\n",
      "test\n",
      "Statistical Features + RF VM test loss 17.214\n",
      "------------------------------------------------\n",
      "all\n",
      "Statistical Features + RF VM test loss 11.18\n",
      "------------------------------------------------\n",
      "extend\n",
      "Statistical Features + RF VM test loss 17.186\n"
     ]
    }
   ],
   "source": [
    "# RF\n",
    "dataset = 'B456'\n",
    "model_type = 'RF'\n",
    "vm_pred = get_VM_predition(dataset, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "unsampled\n",
      "Statistical Features + KNN VM test loss 14.48\n",
      "------------------------------------------------\n",
      "test\n",
      "Statistical Features + KNN VM test loss 30.465\n",
      "------------------------------------------------\n",
      "all\n",
      "Statistical Features + KNN VM test loss 21.882\n",
      "------------------------------------------------\n",
      "extend\n",
      "Statistical Features + KNN VM test loss 30.326\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "dataset = 'B456'\n",
    "model_type = 'KNN'\n",
    "vm_pred = get_VM_predition(dataset, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "unsampled\n",
      "Statistical Features + MLP VM test loss 13.016\n",
      "------------------------------------------------\n",
      "test\n",
      "Statistical Features + MLP VM test loss 18.81\n",
      "------------------------------------------------\n",
      "all\n",
      "Statistical Features + MLP VM test loss 15.699\n",
      "------------------------------------------------\n",
      "extend\n",
      "Statistical Features + MLP VM test loss 18.759\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "dataset = 'B456'\n",
    "model_type = 'MLP'\n",
    "vm_pred = get_VM_predition(dataset, model_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wuenv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
